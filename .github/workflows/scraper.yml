name: First Scraper

# when will this action be triggered
on:
  workflow_dispatch: # dropdown menu for manual runs
    inputs: # creates an input field in the manual run button
      state: 
        description: "U.S. state to scrape"
        required: true 
        # we can create options to specify the type of thing we want to scrape
        default: 'la'
  schedule:
  - cron: "0 0 * * *" # schedule a time (github in UTC)

# allows github to write to files
permissions:
  contents: write

jobs:
  scrape:
    name: Scrape
    runs-on: ubuntu-latest
    steps:
      - name: Checkout # tells the computer to use your repo code
        uses: actions/checkout@v4 
      # preexisting action we can include to get the code (like a python lib)

      - name: Install Python # second step we want computer to do
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # This is where I could run my own script, install the other packages you'd need
      # e.g. python script.py
      - name: Install scraper
        run: pip install warn-scraper # big local news pkg for layoff notices

      # big local news is set up to run in command line
      - name: Scrape
        run: warn-scraper ${{ inputs.state }} --data-dir ./data/ # scrapes iowa data, adds to data folder 
        # ${{ ... }} syntax refers to the inputs set up in workflow dispatch

      - name: Save datestamp # ensures that github does not stop running repo after a period of inactivity
        run: date > ./data/latest-scrape.txt

      # need to commit this information to save to our blank computer
      - name: Commit and push
        run: |
          # the below is completely changeable
          git config user.name "GitHub Actions"
          git config user.email "actions@users.noreply.github.com"
          git add ./data/
          git commit -m "Latest data for ${{ inputs.state }}" && git push || true
          # push || true (a trick so run won't fail if there's nothing new)
          
